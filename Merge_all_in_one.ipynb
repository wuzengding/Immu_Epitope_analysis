{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "708f6436-58a8-43a7-a5ac-0802fef34f95",
   "metadata": {},
   "source": [
    "## config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f9af32c5-609b-43fa-84e9-227a1e036247",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4fded304-7e4d-4a0f-8a7f-4a8c414a6314",
   "metadata": {},
   "outputs": [],
   "source": [
    "## config the path where multiple files contained.\n",
    "## NEED modify every time it runs !!!\n",
    "\n",
    "path = \"/home/jovyan/work/02.ResDev/13.Immuo_Reg_Target_epitope_mouse/mouse_2023-09-13-11\"\n",
    "homobed_or_blasp = \"homoblastp\"\n",
    "'''\n",
    "select list is \"homobed\",\"homoblastp\",\"bedblastp\"\n",
    "\"homobed\" means homobed file as input only\n",
    "\"homoblastp\" means homoblastp file as input only\n",
    "\"bedblastp\" means homobed and homoblastp files both as input\n",
    "'''\n",
    "identity = 65"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f26d716-39c6-4155-b854-ddd8315f399e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## config input files under the given path.\n",
    "## No NEED modify every time it runs !!!\n",
    "\n",
    "path_mhcI = path + \"/02.protein_antigen_prediction/parsed_res_MHCI\"\n",
    "path_mhcII = path + \"/02.protein_antigen_prediction/parsed_res_MHCII\"\n",
    "APres_FIONA = path + \"/02.protein_antigen_prediction/peptide.antigen_presentation.FIONA.seq\"\n",
    "trans_memb = path + \"/04.TransMembrane.DeepTMHMM/TransMembrane.bed\"\n",
    "enzymedigest = path + \"/05.EnzymeDigest/protein.merge.enzymedigest\"\n",
    "\n",
    "if homobed_or_blasp==\"homobed\":\n",
    "    homo_bed = path + \"/03.homologous/homo_peptide.bed\"\n",
    "elif homobed_or_blasp== \"homoblastp\":\n",
    "    homo_blastp = path + \"/03.homologous/peptide_netMHC.blastp\"\n",
    "elif homobed_or_blasp== \"bedblastp\":\n",
    "    homo_bed = path + \"/03.homologous/homo_peptide.bed\"\n",
    "    homo_blastp = path + \"/03.homologous/peptide_netMHC.blastp\"\n",
    "\n",
    "outpath = path + \"/06.Deliverables/EpitopePresent\"\n",
    "outpath1 = path + \"/06.Deliverables/EpitopePresent/MHCI\"\n",
    "outpath2 = path + \"/06.Deliverables/EpitopePresent/MHCII\"\n",
    "try:\n",
    "    os.mkdir(outpath)\n",
    "except:\n",
    "    pass\n",
    "try:\n",
    "    os.mkdir(outpath1)\n",
    "except:\n",
    "    pass\n",
    "try:\n",
    "    os.mkdir(outpath2)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "261617f8-8d47-40d4-9614-2ab7215de8df",
   "metadata": {},
   "source": [
    "#### Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51cc1f2d-ecbb-4d7f-a4d8-a017f64dc1ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(39, 'GGCTCCCCACGGGGTACCCATAACTTGACAGTAGATCTC')\n"
     ]
    }
   ],
   "source": [
    "import difflib\n",
    "def get_overlap(s1, s2):\n",
    "    s = difflib.SequenceMatcher(None, s1, s2)\n",
    "    pos_a, pos_b, size = s.find_longest_match(0, len(s1), 0, len(s2)) \n",
    "    return(size,s1[pos_a:pos_a+size])\n",
    "s1 = \"CGATTCCAGGCTCCCCACGGGGTACCCATAACTTGACAGTAGATCTC\"\n",
    "s2 = \"GGCTCCCCACGGGGTACCCATAACTTGACAGTAGATCTCGTCCAGACCCCTAGC\"\n",
    "\n",
    "print(get_overlap(s1, s2)) # GGCTCCCCACGGGGTACCCATAACTTGACAGTAGATCTC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a6a9f2c-4e12-4e74-93f6-c7367f0af142",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_homodict_bed(Homo_parsDict,seqid,pos1,pos2):\n",
    "    checklist = [homo_f==0 for homo_f in Homo_parsDict[seqid][pos1:pos2]]\n",
    "    if  sum(checklist)/(pos2-pos1) >= 0.05:\n",
    "        HomoEx = \"N\"\n",
    "        Homo_flag = \"\"\n",
    "    else:\n",
    "        HomoEx = \"Y\"\n",
    "        flagtemp_list = []\n",
    "        for flag in Homo_parsDict[seqid][pos1:pos2]:\n",
    "            if flag !=0:\n",
    "                flagtemp_list += flag\n",
    "        Homo_flag = \";\".join(list(dict.fromkeys(flagtemp_list)))\n",
    "    return(HomoEx,Homo_flag)\n",
    "\n",
    "def get_homodict_blastp(Homoblastp_parsDict,seqid,peptide):\n",
    "    try:\n",
    "        homolist = Homoblastp_parsDict[seqid][peptide]\n",
    "        if len(homolist) == 0:\n",
    "            Homo_flag = \"\"\n",
    "            HomoEx = \"N\"\n",
    "        else:\n",
    "            Homo_flag = ';'.join(homolist)\n",
    "            HomoEx = \"Y\"\n",
    "    except:\n",
    "        Homo_flag = \"\"\n",
    "        HomoEx = \"N\"\n",
    "    return(HomoEx,Homo_flag)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c7a83e-3d7c-4ea0-998c-02b336d0bd22",
   "metadata": {},
   "source": [
    "## Step1: Parser the TransMemb.bed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd475435-2a50-41ad-8d2b-077906ba6c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Parser the TransMemb.bed\n",
    "TM_parsDict={}\n",
    "flagDict={\"o\":\"outside\",\"T\":\"TMhelix\",\"i\":\"inside\",\"s\":\"signal\"}\n",
    "'''\n",
    "    parser the tranmembe.bed file\n",
    "    TM_flag=outside,TMhelix,inside\n",
    "    outside:o\n",
    "    TMhelix:t\n",
    "    inside:i\n",
    "'''\n",
    "with open(trans_memb,\"r\") as tmf:\n",
    "    for line in tmf.readlines():\n",
    "        if line.startswith(\"sp|\"):\n",
    "            #seqid = line.split(\"\\t\")[0].split(\"|\")[1]\n",
    "            seqid = line.split(\"\\t\")[0].split(\"|\")[1].split(\"_\")[0]\n",
    "            if seqid not in TM_parsDict.keys():\n",
    "                TM_parsDict[seqid] = \"\"\n",
    "            pos1 = int(line.split(\"\\t\")[1])\n",
    "            pos2 = int(line.split(\"\\t\")[2])\n",
    "            TM_flag = line.split(\"\\t\")[3][0]\n",
    "            TM_parsDict[seqid] += TM_flag*(1+pos2-pos1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "27c2a07b-3aba-48c3-9535-452faa0c7cf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'P09581': 'sssssssssssssssssssooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooTTTTTTTTTTTTTTTTTTTTiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiii', 'Q99NH8': 'ssssssssssssssssssoooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooTTTTTTTTTTTTTTTTTTTTTTiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiii', 'P10923': 'ssssssssssssssssoooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooo', 'A0A087WRT7': 'iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiTTTTTTTTTTTTTTTTTTTTTooooooooooooTTTTTTTTTTTTTTTTTTTTTTiiiiiiiiiiiTTTTTTTTTTTTTTTTTTTTTTTTooooooooooooooooooooooTTTTTTTTTTTTTTTTTTTTTTTiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiii', 'Q8VCH2': 'ssssssssssssssssssoooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooTTTTTTTTTTTTTTTTTTTTTTTiiiiiiiiiiiiiiiiiiiiiii'}\n"
     ]
    }
   ],
   "source": [
    "print(TM_parsDict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee0c525-c8fc-4912-9dba-2c4dc2b6f6e4",
   "metadata": {},
   "source": [
    "## Step2: Parser the Homologous_peptide.bed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "223fc787-7c35-4d15-85a8-01f1a46ffee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Parser the Homologous_peptide.bed\n",
    "if (homobed_or_blasp == \"homobed\") or (homobed_or_blasp == \"bedblastp\"):\n",
    "    Homobed_parsDict={}\n",
    "    with open(homo_bed,\"r\") as hmf:\n",
    "        for line in hmf.readlines():\n",
    "            if line.startswith(\"sp|\"):\n",
    "                #print(line)\n",
    "                seqid = line.split(\"\\t\")[0].split(\"|\")[1]\n",
    "                if seqid not in Homobed_parsDict.keys():\n",
    "                    length = len(TM_parsDict[seqid])\n",
    "                    Homobed_parsDict[seqid] = [0]*length\n",
    "                pos1 = int(line.split(\"\\t\")[1])-1\n",
    "                pos2 = int(line.split(\"\\t\")[2])-1\n",
    "                homoid = line.split(\"\\t\")[3]\n",
    "                for pos in range(pos1,pos2):\n",
    "                    #print(pos)\n",
    "                    if Homobed_parsDict[seqid][pos] == 0:\n",
    "                        Homobed_parsDict[seqid][pos] = [homoid]\n",
    "                    else:\n",
    "                        Homobed_parsDict[seqid][pos].append(homoid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "533727bd-c95a-45d7-af8f-890cafaa40f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n",
      "['P09581']\n",
      "['P10923']\n",
      "['Q8VCH2']\n",
      "['Q99NH8']\n",
      "['P09581', 'P10923', 'Q8VCH2', 'Q99NH8']\n",
      "nan\n",
      "A0A087WRT7\n",
      "P09581\n",
      "P10923\n",
      "Q8VCH2\n",
      "Q99NH8\n"
     ]
    }
   ],
   "source": [
    "## Parser the peptide_netMHC.blastp\n",
    "if (homobed_or_blasp == \"homoblastp\") or (homobed_or_blasp == \"bedblastp\"):\n",
    "    Homoblastp_parsDict={}\n",
    "    headname= [\"qseqid\",\"sseqid\",\"pident\",\"length\",\"mismatch\",\"gapopen\",\"qstart\",\"qend\",\"sstart\",\"send\",\"evalue\",\"bitscore\"]\n",
    "    blastdf = pd.read_csv(homo_blastp,sep=\"\\t\",names=headname)\n",
    "    blastdf[[\"protein\",\"peptide\"]] = blastdf.qseqid.str.split(\":\",expand=True)\n",
    "    #print(blastdf.head(10))\n",
    "    blastdf[\"proteinUnipID\"] = blastdf.protein.str.split(\"_\").str[1]\n",
    "    #print(blastdf.proteinUnipID.unique().tolist())\n",
    "    proteinlist = blastdf.proteinUnipID.unique().tolist()\n",
    "    selfUniproList = []\n",
    "    for proteinid in proteinlist:\n",
    "        # calculate the self \n",
    "        qblastdf = blastdf[blastdf[\"proteinUnipID\"]==proteinid]\n",
    "        homoRefseries = qblastdf.sseqid.value_counts()\n",
    "        # calculate the query number (or bin numbers)\n",
    "        querynum = len(qblastdf.qseqid.unique())\n",
    "        # find those reference id which is the query ploymer \n",
    "        selfUniproID = homoRefseries[homoRefseries>querynum*0.9].index.tolist()\n",
    "        selfUniproList += selfUniproID\n",
    "        print(selfUniproID)\n",
    "    print(selfUniproList)\n",
    " \n",
    "    # remove those aligned lines which reference is it's self\n",
    "    noSelfBlastdf = blastdf[~blastdf.sseqid.isin(selfUniproList)]\n",
    "    # select those aligned lines which pident bigger than cutoff\n",
    "    noSelfHomodf = noSelfBlastdf[noSelfBlastdf.pident > identity] # identity=65\n",
    "    \n",
    "    noSelfHomodf2 = noSelfHomodf[[\"proteinUnipID\",\"peptide\",\"sseqid\"]]\n",
    "    #print(noSelfHomodf2.head(20))\n",
    "    for proteinid in proteinlist:\n",
    "        print(proteinid)\n",
    "        noSelfHomodf2_p = noSelfHomodf2[noSelfHomodf2[\"proteinUnipID\"]==proteinid]\n",
    "        peptidelist = noSelfHomodf2_p.peptide.unique().tolist()\n",
    "        peptidesseqiddict = {}\n",
    "        for peptide in peptidelist:\n",
    "            sseqidlist = noSelfHomodf2_p[noSelfHomodf2_p[\"peptide\"]==peptide].sseqid.tolist()\n",
    "            peptidesseqiddict[peptide] = sseqidlist\n",
    "            Homoblastp_parsDict[proteinid] = peptidesseqiddict\n",
    "    #print(Homoblastp_parsDict)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36cb1d42-1edc-4de7-90ac-9099a58633f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ec25b4e3-6e3f-4b31-8567-4b5b42df6e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(Homoblastp_parsDict[\"Q96A99\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9300eaed-b695-4c94-8adf-543b1a94d99c",
   "metadata": {},
   "source": [
    "## Step3: Parser the enzymedigest file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2cac6839-b048-48c8-b400-250efe4dfd06",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Parser the enzymedigest file\n",
    "##适用于TAA的分析\n",
    "EnzDigeDict={}\n",
    "with open(enzymedigest,\"r\") as enzymef:\n",
    "    for line in enzymef.readlines():\n",
    "        refseq = re.split(',\"\\[',line)[0].split(\",\")[-1]\n",
    "        refid = re.split(',\"\\[',line)[0].split(\"|\")[1].split(\"_\")[0]\n",
    "        seqlist = re.split(',\"\\[',line)[1].strip().replace(']\"','').replace(\"'\",\"\").split(\",\")\n",
    "        EnzDigeDict[refid]=seqlist\n",
    "#print(EnzDigeDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "80e4d298-27d6-4f4a-bff8-80ffbd7e5817",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n"
     ]
    }
   ],
   "source": [
    "print(EnzDigeDict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc34cc7a-5670-4523-845c-0f332e303583",
   "metadata": {},
   "source": [
    "## Step4: Parse the Antigen Presentation of FIONA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dd962990-6270-4564-bcae-16692e7ef88d",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/jovyan/work/02.ResDev/13.Immuo_Reg_Target_epitope_mouse/mouse_2023-09-13-11/02.protein_antigen_prediction/peptide.antigen_presentation.FIONA.seq'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m## Parser the AP result of FIONA\u001b[39;00m\n\u001b[1;32m      2\u001b[0m AP_Fiona \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mAPres_FIONA\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m APresFiona:\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m APresFiona\u001b[38;5;241m.\u001b[39mreadlines():\n\u001b[1;32m      5\u001b[0m         \u001b[38;5;66;03m#print(line)\u001b[39;00m\n\u001b[1;32m      6\u001b[0m         line \u001b[38;5;241m=\u001b[39m line\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m|\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mrstrip()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/IPython/core/interactiveshell.py:284\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    279\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    280\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    282\u001b[0m     )\n\u001b[0;32m--> 284\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/jovyan/work/02.ResDev/13.Immuo_Reg_Target_epitope_mouse/mouse_2023-09-13-11/02.protein_antigen_prediction/peptide.antigen_presentation.FIONA.seq'"
     ]
    }
   ],
   "source": [
    "## Parser the AP result of FIONA\n",
    "AP_Fiona = {}\n",
    "with open(APres_FIONA,\"r\") as APresFiona:\n",
    "    for line in APresFiona.readlines():\n",
    "        #print(line)\n",
    "        line = line.replace(\"|\",\"_\").rstrip()\n",
    "        #print(line)\n",
    "        seqid = line.split(\"_\")[1]\n",
    "        peptide = line.split(\",\")[1]\n",
    "        genetype = line.split(\",\")[2].replace(\"-\",\"\")\n",
    "        Apres = line.split(\",\")[4]\n",
    "        \n",
    "        if seqid not in AP_Fiona.keys():\n",
    "            AP_Fiona[seqid] = {}\n",
    "            AP_Fiona[seqid][\":\".join([peptide,genetype])] = Apres\n",
    "        else:\n",
    "            AP_Fiona[seqid][\":\".join([peptide,genetype])] = Apres\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "267fffee-9e6e-491f-8b37-31685533f495",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['TYRP2', 'TYRP1', 'PMEL'])\n"
     ]
    }
   ],
   "source": [
    "print(AP_Fiona.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346a001c-8f73-40a8-b885-8fd0141b889d",
   "metadata": {},
   "source": [
    "## Step5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2b22cd10-ad25-4df1-b01f-e41225f4f4e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sp_A0A087WRT7_A_MHCI_H-2-Db.netMHC.csv\n",
      "sp_Q99NH8_TREM2_MHCI_H-2-Kb.netMHC.csv\n",
      "sp_P09581_CSF1R_MHCI_H-2-Db.netMHC.csv\n",
      "sp_Q8VCH2_CLM5__MHCI_H-2-Db.netMHC.csv\n",
      "sp_Q8VCH2_CLM5__MHCI_H-2-Kb.netMHC.csv\n",
      "sp_A0A087WRT7_A_MHCI_H-2-Kb.netMHC.csv\n",
      "sp_Q99NH8_TREM2_MHCI_H-2-Db.netMHC.csv\n",
      "sp_P09581_CSF1R_MHCI_H-2-Kb.netMHC.csv\n",
      "sp_P10923_OSTP__MHCI_H-2-Kb.netMHC.csv\n",
      "sp_P10923_OSTP__MHCI_H-2-Db.netMHC.csv\n"
     ]
    }
   ],
   "source": [
    "## Parser the netMHC.csv\n",
    "mergeoutfile =  open(outpath+\"/MHCI_Epitopeonly_SB_WB.csv\",\"w\")\n",
    "filenum = 0\n",
    "for file in glob.glob(path_mhcI + \"/*.netMHC.csv\"):\n",
    "    filenum +=1\n",
    "    filename= os.path.basename(file)\n",
    "    print(filename)\n",
    "    outfile = open(outpath1+\"/\"+filename,\"w\")\n",
    "    with open(file,\"r\") as eptf:\n",
    "        for line in eptf.readlines():\n",
    "            #print(line)\n",
    "            itemlist = line.rstrip().split(\",\")\n",
    "            \"\"\"\n",
    "            itemlist is the line content of netMHC result\n",
    "            \"\"\"\n",
    "            if line.startswith(\"Pos\"):\n",
    "                appendlist = [\"AP_result_of_FIONA\",\"TransMemb\",\"InCutmerRate\",\"InCutmerRegion\",\"HomoExsit\",\"HomoId\"]\n",
    "                outfile.write(\",\".join(itemlist+appendlist)+\"\\n\")\n",
    "                if filenum == 1:\n",
    "                    mergeoutfile.write(\",\".join(itemlist+appendlist)+\"\\n\")\n",
    "            else:\n",
    "                #print(itemlist[0])\n",
    "                pos1 = int(itemlist[0])-1\n",
    "                peptide = itemlist[2]\n",
    "                pos2 = pos1 + len(peptide)\n",
    "                #print(itemlist)\n",
    "                seqid = itemlist[10].split(\"_\")[1].split(\"_\")[0]\n",
    "                midpos = int((pos1+pos2)/2)\n",
    "                genetype =  itemlist[1].replace(\"-\",\"\")\n",
    "                \n",
    "                #print(seqid)\n",
    "                \n",
    "                ## get AP result of FIONA\n",
    "                #APres_Fiona = AP_Fiona[seqid][\":\".join([peptide,genetype])]\n",
    "                APres_Fiona = \"-\"\n",
    "                \n",
    "                ## get Transmembrane info\n",
    "                TM_f = TM_parsDict[seqid][midpos]\n",
    "                TM_flag = flagDict[TM_f]\n",
    "                \n",
    "                ## get enzymedigest info\n",
    "                size0 = 0\n",
    "                overseq0 = \"\"\n",
    "                '''\n",
    "                for seq in EnzDigeDict[seqid]:\n",
    "                    size,overseq = get_overlap(peptide,seq)\n",
    "                    if size > size0:\n",
    "                        size0 = size\n",
    "                        overseq0 = overseq\n",
    "                Cut_flag = '\"'+str(size0)+\"//\"+str(len(peptide))+'\"'\n",
    "                Cut_seq = overseq0\n",
    "                '''\n",
    "                Cut_flag = '-'\n",
    "                Cut_seq = '-'\n",
    "                ## get homolgous info\n",
    "                if (homobed_or_blasp == \"homobed\") or (homobed_or_blasp == \"bedblastp\"):\n",
    "                    HomoEx,Homo_flag = get_homodict_bed(Homo_parsDict,seqid,pos1,pos2)\n",
    "                elif (homobed_or_blasp == \"homoblastp\") or (homobed_or_blasp == \"bedblastp\"):\n",
    "                    HomoEx,Homo_flag = get_homodict_blastp(Homoblastp_parsDict,seqid,peptide)\n",
    "                    \n",
    "                ##  output the results with filtering the \"SB\" lines\n",
    "                if (itemlist[-1] == \"SB\") or (itemlist[-1] == \"WB\"):\n",
    "                    appendlist = [APres_Fiona,TM_flag,Cut_flag,Cut_seq,HomoEx,Homo_flag]\n",
    "                    mergeoutfile.write(\",\".join(itemlist+appendlist)+\"\\n\")\n",
    "                else:\n",
    "                    appendlist = [\"\",APres_Fiona,TM_flag,\"\",\"\",HomoEx,Homo_flag]\n",
    "                outfile.write(\",\".join(itemlist+appendlist)+\"\\n\")\n",
    "    outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0cbf63f5-f091-4482-91d3-0e18bf041e9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TYRP1\n"
     ]
    }
   ],
   "source": [
    "print(seqid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6a9d9a9b-65b7-401c-8988-3ab3a31decbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sp_Q99NH8_TREM2_MHCII_H-2-IAb.netMHC.csv\n",
      "sp_P09581_CSF1R_MHCII_H-2-IAb.netMHC.csv\n",
      "sp_Q8VCH2_CLM5__MHCII_H-2-IAb.netMHC.csv\n",
      "sp_P10923_OSTP__MHCII_H-2-IAb.netMHC.csv\n",
      "sp_A0A087WRT7_A_MHCII_H-2-IAb.netMHC.csv\n"
     ]
    }
   ],
   "source": [
    "## Parser the netMHC.csv\n",
    "mergeoutfile2 =  open(outpath+\"/MHCII_Epitope_only_SB_WB.csv\",\"w\")\n",
    "filenum = 0\n",
    "for file in glob.glob(path_mhcII + \"/*.netMHC.csv\"):\n",
    "    filenum += 1\n",
    "    filename= os.path.basename(file)\n",
    "    print(filename)\n",
    "    outfile = open(outpath2+\"/\"+filename,\"w\")\n",
    "    with open(file,\"r\") as eptf:\n",
    "        for line in eptf.readlines():\n",
    "            itemlist = line.rstrip().split(\",\")\n",
    "            #print(itemlist)\n",
    "            if line.startswith(\"Pos\"):\n",
    "                appendlist = [\"AP_result_of_FIONA\",\"TransMemb\",\"InCutmerRate\",\"InCutmerRegion\",\"HomoExsit\",\"HomoId\"]\n",
    "                outfile.write(\",\".join(itemlist+appendlist)+\"\\n\")\n",
    "                if filenum == 1:\n",
    "                    mergeoutfile2.write(\",\".join(itemlist+appendlist)+\"\\n\")\n",
    "            else:\n",
    "                pos1 = int(itemlist[0])-1\n",
    "                peptide = itemlist[2]\n",
    "                pos2 = pos1 + len(peptide)\n",
    "                seqid = itemlist[6].split(\"|\")[1].split(\"_\")[0]\n",
    "                midpos = int((pos1+pos2)/2)\n",
    "                genetype =  itemlist[1].replace(\"-\",\"\")\n",
    "                \n",
    "                ## get AP result of FIONA\n",
    "                #APres_Fiona = AP_Fiona[seqid][\":\".join([peptide,genetype])]              \n",
    "                APres_Fiona = \"-\"              \n",
    "                    \n",
    "                ## get Transmembrane info\n",
    "                TM_f = TM_parsDict[seqid][midpos]\n",
    "                TM_flag = flagDict[TM_f]\n",
    "                \n",
    "                ## get enzymedigest info\n",
    "                size0 = 0\n",
    "                overseq0 = \"\"\n",
    "                '''\n",
    "                for seq in EnzDigeDict[seqid]:\n",
    "                    size,overseq = get_overlap(peptide,seq)\n",
    "                    if size > size0:\n",
    "                        size0 = size\n",
    "                        overseq0 = overseq\n",
    "                Cut_flag = '\"'+str(size0)+\"//\"+str(len(peptide))+'\"'\n",
    "                Cut_seq = overseq0\n",
    "                '''\n",
    "                Cut_flag = '-'\n",
    "                Cut_seq = '-'\n",
    "                ## get homolgous info\n",
    "                if (homobed_or_blasp == \"homobed\") or (homobed_or_blasp == \"bedblastp\"):\n",
    "                    HomoEx,Homo_flag = get_homodict_bed(Homo_parsDict,seqid,pos1,pos2)\n",
    "                elif (homobed_or_blasp == \"homoblastp\") or (homobed_or_blasp == \"bedblastp\"):\n",
    "                    HomoEx,Homo_flag = get_homodict_blastp(Homoblastp_parsDict,seqid,peptide)\n",
    "                        \n",
    "                ##  output the results with filtering the \"SB\" lines\n",
    "                if (itemlist[-1] == \"SB\") or (itemlist[-1] == \"WB\"):\n",
    "                    appendlist = [APres_Fiona,TM_flag,Cut_flag,Cut_seq,HomoEx,Homo_flag]\n",
    "                    mergeoutfile2.write(\",\".join(itemlist+appendlist)+\"\\n\")\n",
    "                else:\n",
    "                    appendlist = [\"\",APres_Fiona,TM_flag,\"\",\"\",HomoEx,Homo_flag]\n",
    "                outfile.write(\",\".join(itemlist+appendlist)+\"\\n\")\n",
    "    outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4e5b61fc-563b-4c34-adc9-4f1e2ee7dea1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sp_A0A087WRT7_A', 'sp_Q99NH8_TREM2', 'sp_P09581_CSF1R', 'sp_Q8VCH2_CLM5_', 'sp_P10923_OSTP_']\n",
      "0    H-2-Db\n",
      "1    H-2-Kb\n",
      "dtype: object\n",
      "0    H-2-Kb\n",
      "1    H-2-Db\n",
      "dtype: object\n",
      "0    H-2-Db\n",
      "1    H-2-Kb\n",
      "dtype: object\n",
      "0    H-2-Db\n",
      "1    H-2-Kb\n",
      "dtype: object\n",
      "0    H-2-Kb\n",
      "1    H-2-Db\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "df_SB_WB = pd.read_csv(outpath+\"/MHCI_Epitopeonly_SB_WB.csv\")\n",
    "proteinlist = list(df_SB_WB[\"Identity\"].unique())\n",
    "print(proteinlist)\n",
    "for seqid in proteinlist:\n",
    "    try:\n",
    "        os.mkdir(outpath+\"/\"+seqid)\n",
    "    except:\n",
    "        pass\n",
    "    seqid_df = df_SB_WB[df_SB_WB[\"Identity\"].str.contains(seqid)]\n",
    "    MHClist= pd.Series(seqid_df[\"MHC\"].unique())\n",
    "    MHClist2 = MHClist.str.replace('\\*', r'\\\\*', regex=True)\n",
    "    print(MHClist2)\n",
    "    #MHClist2 = MHClist\n",
    "    \n",
    "    for MHCid,MHCid2 in zip(MHClist,MHClist2):\n",
    "        seqid_mhcid_df = seqid_df[seqid_df[\"MHC\"].str.contains(MHCid2)]\n",
    "        seqid_mhcid_df.to_csv(outpath+\"/\"+seqid+\"/\"+seqid+\"_MHCI_\"+MHCid+\".netMHC.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0a46de6a-bbba-46de-8688-23e41edd2615",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_SB_WB = pd.read_csv(outpath+\"/MHCII_Epitope_only_SB_WB.csv\")\n",
    "df_SB_WB[\"Identity\"] = df_SB_WB[\"Identity\"].str.replace(\"|\",\"_\")\n",
    "proteinlist = list(df_SB_WB[\"Identity\"].unique())\n",
    "for seqid in proteinlist:\n",
    "    try:\n",
    "        os.mkdir(outpath+\"/\"+seqid2)\n",
    "    except:\n",
    "        pass\n",
    "    seqid_df = df_SB_WB[df_SB_WB[\"Identity\"].str.contains(seqid)]\n",
    "    MHClist= (seqid_df[\"MHC\"].unique())\n",
    "    for MHCid in MHClist:\n",
    "        seqid_mhcid_df = seqid_df[seqid_df[\"MHC\"].str.contains(MHCid)]\n",
    "        seqid_mhcid_df.to_csv(outpath+\"/\"+seqid+\"/\"+seqid+\"_MHCII_\"+MHCid+\".netMHC.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d5ad15-0532-4e9b-952f-b858a027fb84",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
